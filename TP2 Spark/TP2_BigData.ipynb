{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark – Cálculo del índice TF-IDF**"
      ],
      "metadata": {
        "id": "Vw7F7AUWyTpg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp3QcbRxkLqR"
      },
      "outputs": [],
      "source": [
        "# Instalar Spark para Python\n",
        "!pip install pyspark\n",
        "\n",
        "import os\n",
        "\n",
        "# Instalar Java SDK 8\n",
        "!apt-get install -y openjdk-8-jdk -qq > /dev/null\n",
        "!echo $(/usr/libexec/java_home -v 1.8)\n",
        "\n",
        "# Set environment variable\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!echo 2 | update-alternatives --config java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPMNgOsXysC6"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive en Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gca77oYkWoB"
      },
      "outputs": [],
      "source": [
        "# Definición de rutas\n",
        "root_path = '/content/drive/MyDrive/Colab/A2/'\n",
        "inputDir = root_path + \"input/\"\n",
        "outputDir = root_path + \"output/\"\n",
        "output_d = outputDir + 'output_d_spark'\n",
        "output_idf_tf = outputDir + 'output_idf_tf_spark'\n",
        "\n",
        "# Inicializa contexto\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"TF-IDF\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos de entrada como un RDD de pares clave-valor\n",
        "input_data = sc.textFile(inputDir + \"dataset.txt\")"
      ],
      "metadata": {
        "id": "4TD9tebSzZKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvYjLVk4oFPX"
      },
      "source": [
        "##Limpieza de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6LoB1C3AXKg"
      },
      "outputs": [],
      "source": [
        "def only_recipes(line):\n",
        "  lenght = len(line)\n",
        "\n",
        "  if lenght >= 2:\n",
        "    texto = line[1]\n",
        "    return not texto.startswith('\"Ingredientes:\"')\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r40SNxoczn0f"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Función para limpiar el texto\n",
        "def clean_text(text):\n",
        "    caracteres_permitidos = r\"A-Za-z0-9\"\n",
        "    words = re.split(r'\\s+', text)\n",
        "    cleaned_text = \"\"\n",
        "\n",
        "    for word in words:\n",
        "        if (word != ',' and word != ' '):\n",
        "          cleaned_word = re.sub(f\"[^({caracteres_permitidos})]\", \"\", word)\n",
        "          cleaned_text += cleaned_word + \" \"\n",
        "\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjSuSXUpR5BO"
      },
      "source": [
        "Función para gurdar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkJNrL2bR7hO"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "def save_results(rdd, path):\n",
        "  # Verificar si el directorio ya existe y eliminarlo si es necesario\n",
        "  if os.path.exists(path):\n",
        "      shutil.rmtree(path)\n",
        "\n",
        "  # Guardar los resultados de TF\n",
        "  rdd.saveAsTextFile(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ZzIymOa1k0fN"
      },
      "outputs": [],
      "source": [
        "# Limpieza de datos\n",
        "\n",
        "# Mapea los archivos a lineas con el formato: Doc_id Texto\n",
        "lines = input_data.map(lambda texto: texto.split(\"\\t\"))\n",
        "\n",
        "# Filtra descartando las lineas las lineas que comiencen con \"Ingredientes\"\n",
        "recipes = lines.filter(only_recipes)\n",
        "\n",
        "# Mapea las lineas de Texto -> Texto limpio\n",
        "cleaned_data = recipes.map(lambda x: (x[0], clean_text(x[1])))\n",
        "\n",
        "def format_for_tf(t):\n",
        "  id_doc = t[0]\n",
        "  words = t[1].split()\n",
        "\n",
        "  for w in words:\n",
        "    yield ((id_doc, w), 1)\n",
        "\n",
        "# Establecer clave (id_doc, termino) y valor 1\n",
        "formated_data = cleaned_data.flatMap(format_for_tf)\n",
        "\n",
        "# Unir múltiples partes con mismo id_doc en un solo documento\n",
        "combined_data = cleaned_data.reduceByKey(lambda x, y: str(x) + \" \" + str(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "KuIDpfzkZGh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4028826-7608-43b2-fc2d-c9b741933b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('1', 'Para'), 1), (('1', 'hacer'), 1), (('1', 'la'), 1), (('1', 'receta'), 1), (('1', 'de'), 1)]\n"
          ]
        }
      ],
      "source": [
        "print(formated_data.take(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD7iso_AMcuE"
      },
      "source": [
        "##Calculo de TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "0DPYbqsIMUlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85409592-dffb-45f1-c09c-38ad16139ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('1', 'Para'), 0.004784688995215311), (('1', 'hacer'), 0.004784688995215311), (('1', 'la'), 0.028708133971291867), (('1', 'receta'), 0.004784688995215311), (('1', 'de'), 0.04784688995215311)]\n"
          ]
        }
      ],
      "source": [
        "# Calcular TF\n",
        "tf_data = formated_data.reduceByKey(lambda x,y: x + y)\n",
        "\n",
        "# Cantidad de palabras por documento (id_doc, cantidad de palabras)\n",
        "words_per_document = combined_data.map(lambda t: (t[0], len(t[1].split())))\n",
        "\n",
        "# Establece la clave como el id_doc y los valores como (termino, cantidad de veces que aparece en el documento)\n",
        "prepare_to_join = tf_data.map(lambda t: (t[0][0], (t[0][1], int(t[1]))))\n",
        "\n",
        "# Unifica la cantidad de veces que aparece cada termino por documento y la cantidad de terminos por documento\n",
        "join_words_with_count = prepare_to_join.join(words_per_document)\n",
        "\n",
        "# Calcula el TF por termino y documento\n",
        "tf = join_words_with_count.map(lambda t: ((t[0], t[1][0][0]), float(t[1][0][1]/float(t[1][1]))))\n",
        "\n",
        "print(tf.take(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw1pRUNxP0cU"
      },
      "source": [
        "##Calculo de D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "EkUXTl2qP6-g"
      },
      "outputs": [],
      "source": [
        "# Calcular D\n",
        "d_count = combined_data.count()\n",
        "d_data = sc.parallelize([(\"D:\", d_count)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "YZApm-mITD-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aeee971-8267-46db-af17-333f75fb4996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('D:', 1908)\n"
          ]
        }
      ],
      "source": [
        "print(d_data.first())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqlnYTen9uso"
      },
      "source": [
        "##Calculo de D(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "jFs10c4C-JAv"
      },
      "outputs": [],
      "source": [
        "# Para cada palabra genero un tupla de la forma <Palabra, 1>\n",
        "words = tf.map(lambda x: (x[0][1], 1))\n",
        "\n",
        "# Contar la cantidad de veces que aparece la clave: <Palabra>\n",
        "word_frequency = words.reduceByKey(lambda x, y: x + y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "KqOT0CTvA2-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72d15b6c-6b21-4ee3-a677-da2731700f4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('en', 1892), ('y', 1908), ('calienta', 227)]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "word_frequency.take(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT2_ASboQaU7"
      },
      "source": [
        "##Calculo de IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "CxQZlZzXAtAI"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "total_documents = d_data.first()[1]\n",
        "\n",
        "# Para cada termino se calcula log(#D / D(t)), donde #D es el total de Documentos y D(t) es la\n",
        "# cantidad de veces que aparece el termino t en al menos uno de los documentos\n",
        "\n",
        "idf_rdd = word_frequency.map(lambda x: (x[0], math.log(total_documents / x[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "Q_y92KvIBP2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74763235-b97f-4ec3-cb89-83c45adb0b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('en', 0.008421102396408345), ('y', 0.0), ('calienta', 2.1288608345268294), ('cucharadas', 1.8702310846695502), ('los', 0.087583295792751)]\n"
          ]
        }
      ],
      "source": [
        "print(idf_rdd.take(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "414oxTt8Hj5B"
      },
      "source": [
        "##Calculo de TF-IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "B6UixzmlHm8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f46642e-abb4-4cb7-f909-bb403f6ab30d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('en', 44.22505751027202), ('y', 76.2798744401357), ('calienta', 1.1176546891416206)]\n"
          ]
        }
      ],
      "source": [
        "# Aplicar la función map al RDD: Dada las tuplas de la forma: Key, ('Palabra', Frecuencia_en_key-esimo_documento), retorna:\n",
        "# 'Palabra(nueva_clave)', frecuencia\n",
        "tf_format = tf.map(lambda t: (t[0][1], float(t[1])))\n",
        "\n",
        "# Sumatorio de los TFs por termino\n",
        "tf_sum = tf_format.reduceByKey(lambda x,y: x + y)\n",
        "\n",
        "print(tf_sum.take(3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unifica la sumatorio de TF por termino junto al IDF del termino\n",
        "join = tf_sum.join(idf_rdd)\n",
        "\n",
        "# Calcula el TF-IDF realizando el producto entre sumatoria(TF(t))*IDF(t)\n",
        "res = join.mapValues(lambda t: t[0] * t[1])"
      ],
      "metadata": {
        "id": "mMxCWcbdRq7n"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guarda el resultado en el directorio \"output_idf_tf\"\n",
        "save_results(res, output_idf_tf)\n",
        "\n",
        "print(res.take(5))"
      ],
      "metadata": {
        "id": "1kBs8gl-NDnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7584f90-ee30-463a-eb4b-2c3e0908263c"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('y', 0.0), ('cucharadas', 2.915782900667218), ('los', 2.7359825398069217), ('hasta', 4.449240748528755), ('pimienta', 3.8295409058454504)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Diferencias entre Spark y MapReduce: Debugeo, Abstracción y Rendimiento\n"
      ],
      "metadata": {
        "id": "57r8DYdDVqv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al llevar a cabo el mismo trabajo con dos frameworks distintos, Spark y MapReduce, pudimos experimentar varias diferencias significativas. Al utilizar MapReduce, notamos que la capacidad de imprimir mensajes de depuración directamente en las funciones de Map o Reduce facilitó enormemente la identificación y resolución de problemas específicos en diferentes etapas del código.\n",
        "\n",
        "Sin embargo, al migrar al uso de Spark, encontramos que no se podían incluir declaraciones de impresión directamente en las funciones de filtro y mapeo. Esta limitación dificultó la identificación rápida de problemas en etapas específicas del proceso de datos.\n",
        "\n",
        "Además, enfrentamos la complejidad adicional de los errores en Spark, donde los mensajes de error a menudo mostraban una pila de errores de Java, lo que complicaba la identificación del origen exacto del problema, especialmente al trabajar con Python.\n",
        "\n",
        "En cuanto al rendimiento y el flujo de trabajo, MapReduce requería la generación de archivos intermedios, lo que ralentizaba la ejecución debido a la escritura y lectura de estos archivos. En contraste, al utilizar Spark, experimentamos una ejecución más rápida y eficiente, ya que mantenía los datos en memoria, minimizando la necesidad de archivos intermedios. Esto no solo aceleró el proceso, sino que también redujo la complejidad del flujo de trabajo.\n",
        "\n",
        "Además, Spark generalmente requiere menos líneas de código debido a sus abstracciones más elevadas, simplificando el desarrollo."
      ],
      "metadata": {
        "id": "q0HaqFz4X5aa"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}